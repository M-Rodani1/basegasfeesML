{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Gas Optimizer - Model Training on Google Colab\n",
    "\n",
    "This notebook trains all 3 prediction models (1h, 4h, 24h) using the collected data.\n",
    "\n",
    "**Steps:**\n",
    "1. Upload `gas_data_colab.db` to Colab (from backend folder)\n",
    "2. Run all cells\n",
    "3. Download the trained model files back to your project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install scikit-learn pandas numpy sqlalchemy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if database is uploaded\n",
    "if not os.path.exists('gas_data_colab.db'):\n",
    "    print(\"âŒ Please upload gas_data_colab.db file to this Colab session\")\n",
    "    print(\"   Click the folder icon on the left, then upload the file\")\n",
    "else:\n",
    "    db_size = os.path.getsize('gas_data_colab.db') / (1024 * 1024)\n",
    "    print(f\"âœ… Database found: {db_size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from database\n",
    "conn = sqlite3.connect('gas_data_colab.db')\n",
    "\n",
    "print(\"ðŸ“Š Loading gas prices...\")\n",
    "gas_df = pd.read_sql_query(\"\"\"\n",
    "    SELECT * FROM gas_prices \n",
    "    ORDER BY timestamp\n",
    "\"\"\", conn)\n",
    "gas_df['timestamp'] = pd.to_datetime(gas_df['timestamp'])\n",
    "\n",
    "print(\"ðŸ“Š Loading onchain features...\")\n",
    "onchain_df = pd.read_sql_query(\"\"\"\n",
    "    SELECT * FROM onchain_features \n",
    "    ORDER BY timestamp\n",
    "\"\"\", conn)\n",
    "onchain_df['timestamp'] = pd.to_datetime(onchain_df['timestamp'])\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(f\"\\nâœ… Loaded data:\")\n",
    "print(f\"   Gas prices: {len(gas_df):,} records\")\n",
    "print(f\"   OnChain features: {len(onchain_df):,} records\")\n",
    "print(f\"   Date range: {gas_df['timestamp'].min()} to {gas_df['timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering function\n",
    "def add_time_features(df):\n",
    "    \"\"\"Add time-based features\"\"\"\n",
    "    df = df.copy()\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    return df\n",
    "\n",
    "def add_lag_features(df, column, lags):\n",
    "    \"\"\"Add lag features for a column\"\"\"\n",
    "    df = df.copy()\n",
    "    for lag in lags:\n",
    "        df[f'{column}_lag_{lag}'] = df[column].shift(lag)\n",
    "    return df\n",
    "\n",
    "def add_rolling_features(df, column, windows):\n",
    "    \"\"\"Add rolling statistics\"\"\"\n",
    "    df = df.copy()\n",
    "    for window in windows:\n",
    "        df[f'{column}_rolling_mean_{window}'] = df[column].rolling(window=window, min_periods=1).mean()\n",
    "        df[f'{column}_rolling_std_{window}'] = df[column].rolling(window=window, min_periods=1).std().fillna(0)\n",
    "    return df\n",
    "\n",
    "print(\"âœ… Feature engineering functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "print(\"ðŸ“Š Preparing training data...\\n\")\n",
    "\n",
    "# Merge gas prices with onchain features\n",
    "df = pd.merge_asof(\n",
    "    gas_df.sort_values('timestamp'),\n",
    "    onchain_df.sort_values('timestamp'),\n",
    "    on='timestamp',\n",
    "    direction='nearest',\n",
    "    tolerance=pd.Timedelta('5min')\n",
    ")\n",
    "\n",
    "print(f\"âœ… Merged data: {len(df):,} samples\")\n",
    "\n",
    "# Add time features\n",
    "df = add_time_features(df)\n",
    "\n",
    "# Add lag features for gas price (1, 2, 3, 6, 12 periods back)\n",
    "df = add_lag_features(df, 'gas_price', [1, 2, 3, 6, 12])\n",
    "\n",
    "# Add rolling statistics (5, 15, 60 minute windows)\n",
    "df = add_rolling_features(df, 'gas_price', [5, 15, 60])\n",
    "\n",
    "# Handle enhanced features if present\n",
    "enhanced_features = ['pending_tx_count', 'unique_addresses', 'tx_per_second', 'gas_utilization_ratio']\n",
    "for feat in enhanced_features:\n",
    "    if feat in df.columns:\n",
    "        df[feat] = df[feat].fillna(df[feat].median())\n",
    "\n",
    "# Add derived congestion features\n",
    "if 'pending_tx_count' in df.columns and 'tx_per_second' in df.columns:\n",
    "    df['congestion_level'] = df['pending_tx_count'] * df['tx_per_second']\n",
    "    df['is_highly_congested'] = ((df['pending_tx_count'] > df['pending_tx_count'].quantile(0.75)) & \n",
    "                                  (df['tx_per_second'] > df['tx_per_second'].quantile(0.75))).astype(int)\n",
    "\n",
    "# Drop rows with NaN (from lag features)\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"âœ… Prepared {len(df):,} samples with {len(df.columns)} features\")\n",
    "print(f\"\\nðŸ“Š Feature columns: {df.columns.tolist()[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prediction targets\n",
    "def create_targets(df, horizons):\n",
    "    \"\"\"Create target variables for different prediction horizons\"\"\"\n",
    "    df = df.copy()\n",
    "    for horizon_name, minutes in horizons.items():\n",
    "        periods = minutes  # Since we're sampling every minute\n",
    "        df[f'target_{horizon_name}'] = df['gas_price'].shift(-periods)\n",
    "    return df\n",
    "\n",
    "horizons = {'1h': 60, '4h': 240, '24h': 1440}\n",
    "df = create_targets(df, horizons)\n",
    "df = df.dropna()  # Remove rows where we can't create targets\n",
    "\n",
    "print(f\"âœ… Created prediction targets for {list(horizons.keys())}\")\n",
    "print(f\"   Final training samples: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns\n",
    "feature_cols = [col for col in df.columns if col not in \n",
    "                ['timestamp', 'id', 'gas_price', 'target_1h', 'target_4h', 'target_24h']]\n",
    "\n",
    "print(f\"ðŸ“Š Using {len(feature_cols)} features for training:\")\n",
    "print(f\"   {feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(X, y, horizon_name):\n",
    "    \"\"\"Train a single model with time-series cross-validation\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸŽ¯ Training model for {horizon_name} prediction horizon\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"   Samples: {len(X):,}\")\n",
    "    print(f\"   Features: {X.shape[1]}\")\n",
    "    \n",
    "    # Scale features using RobustScaler (handles outliers better)\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    print(\"âœ… Features scaled with RobustScaler\")\n",
    "    \n",
    "    # Time-series cross-validation\n",
    "    print(\"ðŸ“Š Running time-series cross-validation...\")\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_scaled), 1):\n",
    "        X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Train RandomForest\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=15,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=5,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Validate\n",
    "        y_pred = model.predict(X_val)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        \n",
    "        # Directional accuracy\n",
    "        direction_correct = np.sum((y_pred > y_train.iloc[-1]) == (y_val.values > y_train.iloc[-1]))\n",
    "        dir_acc = direction_correct / len(y_val)\n",
    "        \n",
    "        cv_scores.append({'mae': mae, 'r2': r2, 'dir_acc': dir_acc})\n",
    "        print(f\"   Fold {fold}: MAE={mae:.4f}, RÂ²={r2:.4f}, Dir Acc={dir_acc:.2%}\")\n",
    "    \n",
    "    # Print average scores\n",
    "    avg_mae = np.mean([s['mae'] for s in cv_scores])\n",
    "    avg_r2 = np.mean([s['r2'] for s in cv_scores])\n",
    "    avg_dir = np.mean([s['dir_acc'] for s in cv_scores])\n",
    "    print(f\"\\nðŸ“Š Cross-validation results:\")\n",
    "    print(f\"   Average MAE: {avg_mae:.4f}\")\n",
    "    print(f\"   Average RÂ²: {avg_r2:.4f}\")\n",
    "    print(f\"   Average Directional Accuracy: {avg_dir:.2%}\")\n",
    "    \n",
    "    # Train final model on all data\n",
    "    print(f\"\\nðŸŽ¯ Training final model on full dataset...\")\n",
    "    final_model = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=15,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    final_model.fit(X_scaled, y)\n",
    "    print(\"âœ… Final model trained\")\n",
    "    \n",
    "    return {\n",
    "        'model': final_model,\n",
    "        'scaler': scaler,\n",
    "        'feature_names': feature_cols,\n",
    "        'metrics': {\n",
    "            'mae': avg_mae,\n",
    "            'r2': avg_r2,\n",
    "            'directional_accuracy': avg_dir\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"âœ… Training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸš€ STARTING MODEL TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "models = {}\n",
    "\n",
    "for horizon_name in ['1h', '4h', '24h']:\n",
    "    X = df[feature_cols]\n",
    "    y = df[f'target_{horizon_name}']\n",
    "    \n",
    "    trained = train_model(X, y, horizon_name)\n",
    "    models[horizon_name] = trained\n",
    "    \n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… ALL MODELS TRAINED SUCCESSFULLY\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary\n",
    "print(\"\\nðŸ“Š TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "for horizon_name, model_data in models.items():\n",
    "    metrics = model_data['metrics']\n",
    "    print(f\"\\n{horizon_name} Model:\")\n",
    "    print(f\"   MAE: {metrics['mae']:.4f} Gwei\")\n",
    "    print(f\"   RÂ² Score: {metrics['r2']:.4f}\")\n",
    "    print(f\"   Directional Accuracy: {metrics['directional_accuracy']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "print(\"\\nðŸ’¾ Saving models...\")\n",
    "\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "for horizon_name, model_data in models.items():\n",
    "    filename = f'saved_models/model_{horizon_name}.pkl'\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model_data, f)\n",
    "    file_size = os.path.getsize(filename) / 1024\n",
    "    print(f\"âœ… Saved {filename} ({file_size:.1f} KB)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ‰ TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nðŸ“¦ Next steps:\")\n",
    "print(\"   1. Download the 'saved_models' folder from Colab\")\n",
    "print(\"   2. Replace the models in your backend/models/saved_models/ directory\")\n",
    "print(\"   3. Restart your backend server to use the new models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Create a zip file for easy download\n",
    "!zip -r models.zip saved_models/\n",
    "print(\"\\nâœ… Created models.zip - download this file from the Files panel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
